- 파티션의 종류
  - input partition
    - spark.conf.set("spark.sql.files.maxPartitionBytes",134217728 ) // 134217728는 128MB
  - output partition
    - coalesce, repartition, partitionBy
  - shuffle partition
    - spark.conf.set("spark.sql.shuffle.partitions", 숫자)
    - disk에 Dataframe을 Partition Column의 유니크한 값으로 sub-directory로써 파티셔닝 할 때 사용
- 
- RDD 생성시 파티션 개수
    - ex) `sc.parallelize(Array(1, 2, 3, 1, 2, 3, 4), 3)` 
    - 두번쨰 파라미터에 파티션 개수 지정 가능
    - 만약 지정하지 않을 경우 설정값 `spark.default.parallelism` 사용
    - `spark.default.parallelism`도 없을 경우 total core를 고려함
    - local 실행시엔 `local[core수]` 에서 명시한 core수가 total core
    - 배포시엔 max(2, total core수) 로 보임. (CoarseGrainedSchedulerBackend 참고)
  
- 궁금한 사항
  - spark.sql.files.maxPartitionBytes와 spark.default.parallelism 중 어떤 것의 우선순위가 더 높을까?
  
- 구조적 API 리파티셔닝
  - `df.repartition(5, col("DEST_COUNTRY_NAME))` 과 같이 특정 컬럼 기준으로 파티셔닝 가능.
  - 특정 컬럼 기준으로 자주 필터링 한다면 자주 필터링 되는 컬럼을 기준으로 파티션 재분배하는게 좋다고 함.
  
- RDD 파티셔닝
  - 데이터가 클러스터 전체에 물리적으로 분산되는 방식 정의 가능
  - 구조적 API와의 가장 큰 차이점은 파티션 함수(사용자 지정 파티셔너)를 파라미터로 사용할 수 있다는 점
  - coalesce: 셔플 방지하면서 파티션 줄일 수 있음 
  - repartition: 셔플 발생할 수 있지만 파티션수를 늘리거나 줄일 수 있음.
  - 사용자 정의 파티셔닝
    - 논리적인 대응책이 없어서, 구조적 API에선 사용자 정의 파티셔너 사용 불가
    - 대표적인 예제는 페이지랭크
      - 페이지랭크는 사용자 정의 파티셔닝을 이용해 클러스터 데이터 배치 구조 제어하고 셔플 회피
      - 페이지 랭크 알고리즘은 구글의 공동 창업자 Larry Page의 이름을 인용한 것으로 얼마나 많은 문서들이 해당 문서를 링크하고 있는지 기초하여 각 문서에 대해서 중요도를 매기는 알고리즘을 말한다
    - RDD에서 사용자 정의 파티셔닝을 적용한 후 Dataframe이나 Dataset으로 변환 가능
    - Partitioner를 확장한 클래스를 구현해야 사용자 정의 파티셔닝 가능
        