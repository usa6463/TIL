## 환경
- Mac OS (M1)
- local
- IntelliJ

## 실행 정보 확인
- app 실행중엔 Spark UI로 확인 가능 (ex. http://localhost:4040)
- app 실행 종료 후엔 spark history server로 실행 정보 확인 가능
  - spark history server 실행을 위해 Local에서 apache-spark 설치
    - ```bash
      brew install apache-spark
        ```
  - spark history server 실행
    - ```bash
      cd /opt/homebrew/Cellar/apache-spark/{spark version}/libexec/sbin
      ```
    - ```bash
      ./start-history-server.sh  
      ```
  - IntelliJ에서 실행하는 app엔 아래와 같은 설정 필요
    - ```scala
      .config("spark.eventLog.enabled", "true")
      .config("spark.eventLog.dir", "/tmp/spark-events")
      ```
      
## 디펜던시 다운받아서 코드에서 바로 실행하는 것과, local spark 환경에서(stand alone) 실행하는것에 어떤 차이가 있는가?
- 코드에서 바로 실행하는건 executor가 driver 밖에 안생김.
- 스파크에서 설정은 두가지 종류가 있는데, 'spark.driver.memory', 'spark.executor.instances'와 같은 배포와 관련이 있는 경우와 'spark.task.maxFailures'와 같은 Spark 런타임 제어 관련 설정이다.
- 배포 관련 설정은 SparkConf에서 설정해도 먹히지 않을 수 있음. 이런 설정은 설정 파일이나 spark-submit에서 설정 줘야함.
- 결론적으로 IntelliJ에서 Pom.xml 통해 디펜던시 다운 받고 run 하는걸론 실행환경을 변경해서 테스트하는게 힘듬
  - 익스큐터 개수를 늘린다던지 메모리 설정을 한다던지... 
- [참고](https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties)